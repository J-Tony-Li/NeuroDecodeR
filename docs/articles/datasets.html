<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Datasets • The Neural Decoding Toolbox in R</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/sandstone/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Datasets">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">The Neural Decoding Toolbox in R</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.0.9003</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/introduction_tutorial.html">Introductory tutorial</a>
    </li>
    <li>
      <a href="../articles/generalization_tutorial.html">Generalization analysis tutorial</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Documentation
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/data_formats.html">Data formats</a>
    </li>
    <li>
      <a href="../articles/datasets.html">Datasets</a>
    </li>
    <li>
      <a href="../articles/NDTr_object_specification.html">NDTr object specification</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/emeyers/NeuroDecodeR">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Datasets</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/emeyers/NeuroDecodeR/blob/master/vignettes/datasets.Rmd"><code>vignettes/datasets.Rmd</code></a></small>
      <div class="hidden name"><code>datasets.Rmd</code></div>

    </div>

    
    
<p>The following datasets can be used with the NeuroDecodeR to learn how to use the package.</p>
<p><br></p>
<div id="zhang-desimone-7-object-dataset" class="section level1">
<h1 class="hasAnchor">
<a href="#zhang-desimone-7-object-dataset" class="anchor"></a>Zhang-Desimone 7 object dataset</h1>
<p>The Zhang-Desimone 7 object dataset was collected by Ying Zhang in <a href="http://mcgovern.mit.edu/principal-investigators/robert-desimone">Bob Desimone’s lab</a> in the McGovern Institute at MIT. The data was used in the supplemental figures in the paper <a href="http://www.pnas.org/content/108/21/8850">Object decoding with attention in inferior temporal cortex, PNAS, 2011</a>.</p>
<p>The data consists of single unit recordings from the 132 neurons in inferior temporal cortex (IT). The recordings were made while a monkey viewed 7 different objects that were presented at three different locations (the monkey was also shown images that consisted of three objects shown simultaneously and had to perform an attention task, however the dataset compiled here only consists of trials when single objects were shown). Each object was presented approximately 20 times at each of the three locations. The data is in <a href="data_formats.html">raster format</a>, and each trial consists of 500 ms of baseline data where a monkey viewed a fixation dot, and 500 ms of data when a monkey viewed one of the 7 different images.</p>
<div id="accessing-the-dataset" class="section level3">
<h3 class="hasAnchor">
<a href="#accessing-the-dataset" class="anchor"></a>Accessing the dataset</h3>
<p>The Zhang-Desimone 7 object dataset comes with the NeuroDecodeR package. Individual raster files can be found in the directory <code>extdata/Zhang_Desimone_7object_raster_data_rda/</code>. Additionally, binned data using 150 ms bins and 50 ms sampling intervals can be found in the file <code>extdata/ZD_150bins_50sampled.Rda</code>. The code below shows how to load a single raster data file, and the binned data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># load a raster format data file</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">raster_dir_name &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/file.path">file.path</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/system.file">system.file</a></span>(<span class="st">"extdata"</span>, <span class="dt">package =</span> <span class="st">"NeuroDecodeR"</span>), <span class="st">"Zhang_Desimone_7object_raster_data_rda"</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">file_name &lt;-<span class="st"> "bp1001spk_01A_raster_data.rda"</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/load">load</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/file.path">file.path</a></span>(raster_dir_name, file_name))</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co"># load the 150 ms binned data</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">binned_file_name &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/system.file">system.file</a></span>(<span class="st">"extdata/ZD_150bins_50sampled.Rda"</span>, <span class="dt">package=</span><span class="st">"NeuroDecodeR"</span>)</a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/load">load</a></span>(binned_file_name)</a></code></pre></div>
<p><br></p>
<p><br></p>
</div>
</div>
<div id="qi-constantinidis-pre-and-post-training-dataset" class="section level1">
<h1 class="hasAnchor">
<a href="#qi-constantinidis-pre-and-post-training-dataset" class="anchor"></a>Qi-Constantinidis pre and post training dataset</h1>
<p>The Qi-Constantinidis pre and post training dataset was collected by Xue-Lian Qi in <a href="http://neuroscience.graduate.wfu.edu/people/christos-constantinidis/">Christos Constantinidis’ lab</a> in Department of Neurobiology and Anatomy, at the Wake Forest University School of Medicine. The data consists of single unit recordings from the prefrontal cortex (PFC) while monkeys passively viewed images (in a feature and spatial configurations), and also after monkey were trained to engaged in a delayed match-to-sample task with these images.</p>
<p><strong>If you plan to use this data please cite the publications below.</strong> More information about the experiments and data analyses can be found in these papers as well.</p>
<p><em>Meyers E, Qi XL, Constantinidis C (2012). Incorporation of new information into prefrontal cortical activity after learning working memory tasks. Proceedings of the National Academy of Sciences, 109:4651-4656.</em></p>
<p><em>Qi X-L, Meyer T, Stanford TR, Constantinidis C (2011) Changes in prefrontal neuronal activity after learning to perform a spatial working memory task. Cereb Cortex 21:2722–2732</em></p>
<div id="accessing-the-dataset-1" class="section level3">
<h3 class="hasAnchor">
<a href="#accessing-the-dataset-1" class="anchor"></a>Accessing the dataset</h3>
<p>The MATLAB versions of the raster files can be downloaded from <a href="http://www.readout.info">www.readout.info</a> and can be converted to R raster format data files using the <code><a href="../reference/convert_matlab_raster_data.html">convert_matlab_raster_data()</a></code> function.</p>
<p><br></p>
<p><br></p>
</div>
</div>
<div id="isik-26-letter-meg-dataset" class="section level1">
<h1 class="hasAnchor">
<a href="#isik-26-letter-meg-dataset" class="anchor"></a>Isik 26 letter MEG dataset</h1>
<p>The Isik 26 letter MEG dataset was collected by Leyla Isik in Tommy Poggio’s lab and the MEG Lab at the McGovern Institute at MIT. The data was used in Figure 2b of the paper: <a href="https://www.physiology.org/doi/full/10.1152/jn.00394.2013">The dynamics of invariant object recognition in the human visual system, J.</a> Neurophys 2014.</p>
<p>The data consists of 306 channel (comprised of 102 magentometers, and 204 planar gradiometers) MEG recordings from an Elekta Neuromag Triux Scanner. One subject was shown 26 black, upper-case letters, on a white background, while their neural response was recorded in the MEG scanner. Each letter was presented approximately 50 times. The data is in raster-format, and each trial consists of 233 ms of baseline data where the subject viewed a fixation cross, followed by 50 ms of data when the subject viewed the image of one letter, and 417 ms of data when they again viewed a fixation cross.</p>
<p>The data is available in two formats – the raw MEG files output by the scanner (.fif format) and preprocessed data in <a href="raster_format.html">raster format</a>. The raw data download also includes a file with raster labels indicating which stimulus was shown in each trial.</p>
<div id="accessing-the-dataset-2" class="section level3">
<h3 class="hasAnchor">
<a href="#accessing-the-dataset-2" class="anchor"></a>Accessing the dataset</h3>
<p>The MATLAB versions of the raster files can be downloaded from <a href="http://www.readout.info">www.readout.info</a> and can be converted to R raster format data files using the <code><a href="../reference/convert_matlab_raster_data.html">convert_matlab_raster_data()</a></code> function.</p>
<p><br></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#zhang-desimone-7-object-dataset">Zhang-Desimone 7 object dataset</a></li>
      <li><a href="#qi-constantinidis-pre-and-post-training-dataset">Qi-Constantinidis pre and post training dataset</a></li>
      <li><a href="#isik-26-letter-meg-dataset">Isik 26 letter MEG dataset</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Ethan Meyers.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
